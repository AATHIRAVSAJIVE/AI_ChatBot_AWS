# -*- coding: utf-8 -*-
"""train.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17vy3nGIpkLrLYqYO1bcj-owTm0a3xkgN
"""

# Import Required Libraries
import json
import random
import numpy as np
import tensorflow as tf
import pickle
import nltk
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.optimizers import SGD
from keras import callbacks
import warnings
warnings.filterwarnings("ignore")  # Suppress all warnings

# Download NLTK Resources
nltk.download('punkt_tab')
nltk.download('punkt')
nltk.download('wordnet')     # For lemmatization

# Initialize Lemmatizer
lemmatizer = WordNetLemmatizer()

# Load Data intents.json
from google.colab import files
uploaded = files.upload()

with open('intents.json', encoding='utf-8-sig') as file:
    intents = json.load(file)

# Confirm the file exists
import os
print(os.listdir())  # Check for 'intents.json'

# Preprocessing
#     - Tokenization
#     - Lemmatization
#     - Class Collection

words = []
classes = []
documents = []
ignore_letters = ['?', '!', '.', ',']

for intent in intents['intents']:
    for pattern in intent['patterns']:

      # Take each word and tokenize it
        word_list = nltk.word_tokenize(pattern)
        words.extend(word_list)

        # Adding documents
        documents.append((word_list, intent['tag']))

        # Adding classes to class list
        if intent['tag'] not in classes:
            classes.append(intent['tag'])

# Lemmatize and lower each word and remove duplicates
words = [lemmatizer.lemmatize(w.lower()) for w in words if w not in ignore_letters]
words = sorted(set(words))

# Sort classes
classes = sorted(set(classes))

# Inspect Preprocessed Data (Debugging Step)

print("\nPreprocessed Data:")
print(f"\nWords:\n{words}")               # Confirming all words are loaded
print(f"\nClasses (Intents):\n{classes}") # Confirming all intents are loaded

# Display total number of (pattern, tag) pairs collected
print(f"\nTotal documents (patterns): {len(documents)}")

# Show total number of unique intent classes
print(f"\nTotal intent classes: {len(classes)}\n{classes}")

# Print vocabulary (unique input words after lemmatization)
print(f"\nTotal unique lemmatized words: {len(words)}\n{words}")

# Save Preprocessed Data
#     - Vocabulary (words)
#     - Labels (classes)

pickle.dump(words, open('words.pkl', 'wb'))
pickle.dump(classes, open('classes.pkl', 'wb'))

# Create Training Data
#     - Bag of Words + One-hot labels

training = []
output_empty = [0] * len(classes)

for doc in documents:
    bag = []    # Initializing bag of words
    word_patterns = [lemmatizer.lemmatize(w.lower()) for w in doc[0]]   # Tokenize, lowercase, and lemmatize each word in the pattern
    bag = [1 if word in word_patterns else 0 for word in words]   # Create a bag-of-words vector: 1 if word exists in the pattern, else 0

    # Output_row is a one-hot encoded vector:
    # '1' for the tag corresponding to the current pattern, '0' for all others
    output_row = list(output_empty)
    output_row[classes.index(doc[1])] = 1
    training.append([bag, output_row])

random.shuffle(training)   # Shuffle the training data to ensure random distribution
training = np.array(training, dtype=object)   # Convert the training data into a NumPy array for compatibility with TensorFlow

# Split the features (X) and labels (Y)
train_x = np.array(list(training[:, 0]))   # Feature vectors (bag of words)
train_y = np.array(list(training[:, 1]))   # One-hot encoded target classes

# Final confirmation after training data (features and labels) is ready
print("\nFinal Training Data:\n")
print(f"Features (X):\n{train_x}\n")   # Input vectors
print(f"Labels (Y):\n{train_y}\n")     # One-hot encoded intent labels

# Build Neural Network Model

# Architecture:
# - Input Layer: 128 neurons with ReLU activation
# - Hidden Layer: 64 neurons with ReLU activation
# - Output Layer: softmax activation with number of neurons equal to number of classes (intents)
# Dropout layers are added after each dense layer to prevent overfitting

model = Sequential()
model.add(Dense(128, input_shape=(len(train_x[0]),), activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(len(train_y[0]), activation='softmax'))

# Display Model Architecture Summary
model.summary()   # Shows each layer, output shape, and total number of parameters

# Compile the Model
# Using Stochastic Gradient Descent (SGD) with Nesterov momentum for better convergence
# Loss function: categorical_crossentropy for multi-class classification

sgd = SGD(learning_rate=0.01, decay=1e-6, momentum=0.9, nesterov=True)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])

"""### Train and Save the Model (Early Stopping Configuration)

-- EarlyStopping is used to halt training when the model stops improving.

-- Set random seeds for reproducibility (Setting a number used to initialize a pseudorandom number generator (PRNG) )

-- This helps prevent overfitting and reduces unnecessary training time.

 - monitor="loss": watch the training loss metric
 - mode="min": stop when the loss stops decreasing
 - patience=5: wait for 5 epochs without improvement before stopping
 - restore_best_weights=True: restore the best model weights (not the last one)


      # Train and Save the Model
      model.fit(train_x, train_y, epochs=200, batch_size=5, verbose=1)
      model.save('chatbot_model.keras')
"""

# Train and Save the Model

# Set seeds for reproducibility
random.seed(42)           # Ensures consistent results across Python’s random module
np.random.seed(42)        # Ensures consistency for NumPy operations
tf.random.set_seed(42)    # Ensures consistent weight init and randomness in TensorFlow

# Define Early Stopping
earlystopping = callbacks.EarlyStopping(
    monitor="loss",               # Monitor the loss during training
    mode="min",                   # Aim to minimize the loss
    patience=5,                   # Stop after 5 epochs with no improvement
    restore_best_weights=True     # Revert to best weights (lowest loss)
)

# Train the model
model.fit(
    train_x, train_y,
    epochs=200,
    batch_size=5,
    callbacks=[earlystopping],   # Early stopping included
    verbose=1
)

# Save the trained model
model.save('chatbot_model.keras')

print("Model trained and saved as chatbot_model.keras")

"""**Model Performance Summary**

Accuracy: Peaked around98.72% at Epoch 55.(Model learned well)

5.48% at Epoch 1 — shows model started with low confidence.

Best Loss: 0.0882 at Epoch 59, shows excellent learning progress

So the chatbot model is well-trained and reliable, the model has learned to classify intents very effectively.
"""

# Check output files
print(os.listdir())  # show model and pkl files

# TEST THE MODEL AFTER TRAINING

# Function to clean up input sentences: tokenize and lowercase
def clean_up_sentence(sentence):
    sentence_words = word_tokenize(sentence)
    sentence_words = [word.lower() for word in sentence_words]
    return sentence_words

# Function to convert a sentence into a Bag of Words (BoW) array
def bow(sentence, words, show_details=True):
    sentence_words = clean_up_sentence(sentence)
    bag = [0] * len(words)
    for s in sentence_words:
        for i, w in enumerate(words):
            if w == s:
                bag[i] = 1
                if show_details:
                    print(f"Found in bag: {w}")
    return np.array(bag)

# Function to classify the intent of a given sentence
def classify(sentence):
    p = bow(sentence, words, show_details=False)
    res = model.predict(np.array([p]))[0]   # Get prediction probabilities
    ERROR_THRESHOLD = 0.25   # Only consider predictions with confidence > 0.25
    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]
    results.sort(key=lambda x: x[1], reverse=True)
    return [(classes[r[0]], r[1]) for r in results]

# Function to get chatbot's response based on predicted intent
def chatbot_response(text):
    intents_predicted = classify(text)
    if not intents_predicted:
        return "Sorry, I didn't understand that."

    for intent in intents['intents']:
        if intent['tag'] == intents_predicted[0][0]:
            return random.choice(intent['responses'])

# Test the chatbot with sample inputs
print(chatbot_response("Hi there!"))
print(chatbot_response("Tell me about your shipping options."))
print(chatbot_response("Where’s my delivery?"))

"""**Built and Trained a Chatbot**

-- Loaded and preprocessed your intents.json dataset.

-- Tokenized, lemmatized, and vectorized the patterns.

-- Created one-hot encoded class labels.

-- Built and trained a Keras model to classify input intents.

-- Used Early Stopping Configuration

-- Tested the chatbot successfully on sample inputs.
"""

from google.colab import files

# Download the files to local machine
files.download('words.pkl')
files.download('classes.pkl')
files.download('chatbot_model.keras')

